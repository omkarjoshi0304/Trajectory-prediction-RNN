# -*- coding: utf-8 -*-
"""DA2_ASGq2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rexusoGVMKnv2VmTZfreCGh44keAVAjT
"""

import os  # Operating system utilities for file/directory manipulation
import numpy as np  # Numerical computations
import pandas as pd  # Data manipulation and analysis
import matplotlib.pyplot as plt  # Plotting library for visualizations
import tensorflow as tf  # Deep learning framework
import zipfile  # For working with ZIP archives
import requests  # HTTP requests (not used in current code, but imported)
from sklearn.model_selection import train_test_split  # To split data into training, validation, test sets
from sklearn.preprocessing import MinMaxScaler  # For scaling numeric data between 0 and 1
from tensorflow.keras.models import Sequential  # Keras sequential model API
from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU  # Neural network layers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint  # Callbacks for training control
from tensorflow.keras.optimizers import Adam  # Optimizer for training neural networks
import gdown  # To download files from Google Drive
import time  # Provides time-related functions
import math  # Mathematical functions (not explicitly used here)
import folium  # For creating interactive maps
from datetime import datetime  # Working with date and time
from tqdm import tqdm  # Progress bar for loops
import pickle  # For saving and loading Python objects (scaler)

# Check and print the number of available GPUs
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

# Function: download_dataset
# Purpose: Attempts to download the T-Drive dataset from Google Drive.
#          If download fails, it calls create_synthetic_data() to generate synthetic data.
def download_dataset():
    """Download the T-Drive dataset or create synthetic data if the download fails."""
    print("Downloading T-Drive dataset...")
    os.makedirs("t_drive_data", exist_ok=True)  # Ensure the target directory exists
    # Placeholder file ID from Google Drive â€“ replace with your actual file ID if available.
    file_id = '1-1RFhcBbaH0BKHAE0wHILMYiMdmCgZWk'
    # Build the URL using the file ID
    url = f'https://drive.google.com/uc?id={file_id}'
    output = 't_drive_data/t_drive_sample.zip'

    try:
        # Download the file using gdown (Google Drive downloader)
        gdown.download(url, output, quiet=False)
        # Extract the downloaded ZIP file to the 't_drive_data' directory
        with zipfile.ZipFile(output, 'r') as zip_ref:
            zip_ref.extractall('t_drive_data')
        print("Dataset downloaded and extracted successfully.")
        return 't_drive_data'
    except Exception as e:
        # If any error occurs during download or extraction, print error and create synthetic data instead.
        print(f"Error downloading dataset: {e}")
        print("Creating synthetic data instead...")
        return create_synthetic_data()

# Function: create_synthetic_data
# Purpose: Generates synthetic taxi trajectory data that mimics the T-Drive dataset structure.

def create_synthetic_data():
    """Creates synthetic taxi trajectory data similar to the T-Drive dataset format."""
    os.makedirs("t_drive_data/synthetic", exist_ok=True)  # Create directory for synthetic data
    num_taxis = 50  # Number of taxis to simulate
    total_trajectories = 1200  # Total number of trajectories across taxis
    all_trajectories = []  # List to hold all trajectory data

    # Loop over each taxi
    for taxi_id in range(1, num_taxis + 1):
        # Determine the number of trajectories for this taxi with some randomness
        num_trajectories = max(20, int(np.random.normal(total_trajectories / num_taxis, 5)))
        for trajectory_id in range(num_trajectories):
            # Random starting coordinates within approximate ranges (simulate Beijing area)
            start_lat = np.random.uniform(39.8, 40.0)
            start_lon = np.random.uniform(116.3, 116.5)
            # Random length of the trajectory (number of points)
            traj_length = np.random.randint(20, 100)
            # Random base timestamp for the start of the trajectory
            base_timestamp = int(datetime(2022, np.random.randint(1, 13), np.random.randint(1, 28),
                                          np.random.randint(0, 24), 0, 0).timestamp())
            traj_data = []  # List to store the points for this trajectory
            current_lat, current_lon = start_lat, start_lon

            # Generate trajectory points with noise and small movements
            for i in range(traj_length):
                current_lat += np.random.normal(0, 0.0005) + np.random.choice([-0.001, 0, 0.001])
                current_lon += np.random.normal(0, 0.0005) + np.random.choice([-0.001, 0, 0.001])
                # Ensure the coordinates remain within certain bounds
                current_lat = max(min(current_lat, 40.05), 39.75)
                current_lon = max(min(current_lon, 116.55), 116.25)
                # Increment timestamp by 60 seconds per point
                timestamp = base_timestamp + i * 60
                traj_data.append([taxi_id, timestamp, current_lat, current_lon])
            # Append all points from this trajectory to the main list
            for point in traj_data:
                all_trajectories.append(point)

    # Create a DataFrame from the trajectory list
    df = pd.DataFrame(all_trajectories, columns=['taxi_id', 'timestamp', 'latitude', 'longitude'])
    # Save the DataFrame as a CSV file
    df.to_csv('t_drive_data/synthetic/taxi_trajectories.csv', index=False)
    print(f"Created synthetic dataset with {len(df)} points from {total_trajectories} trajectories")
    print(f"Number of unique taxis: {df['taxi_id'].nunique()}")
    return 't_drive_data/synthetic'
# Function: load_data
# Purpose: Loads the dataset from disk.
#          For synthetic data, it reads from the CSV file.
#          For real data (if downloaded), it processes .txt files.

def load_data(data_path):
    """Loads and preprocesses the T-Drive dataset."""
    if 'synthetic' in data_path:
        # If synthetic data, load the CSV file
        df = pd.read_csv(os.path.join(data_path, 'taxi_trajectories.csv'))
    else:
        # For a real dataset, list all text files and process them
        all_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith('.txt')]
        dfs = []
        # Process a subset of files (first 50) for manageability
        for file in tqdm(all_files[:50]):
            try:
                # Read the file with no header and assign column names as per T-Drive format
                taxi_df = pd.read_csv(file, header=None,
                                      names=['taxi_id', 'datetime', 'longitude', 'latitude'],
                                      sep=',')
                # Convert datetime strings to Unix timestamps
                taxi_df['timestamp'] = pd.to_datetime(taxi_df['datetime']).astype(int) // 10**9
                dfs.append(taxi_df)
            except Exception as e:
                print(f"Error loading file {file}: {e}")
                continue
        # Concatenate all dataframes into a single DataFrame
        df = pd.concat(dfs, ignore_index=True)
    return df


# Function: preprocess_data
# Purpose: Cleans and normalizes the data.
#          - Sorts the data by taxi_id and timestamp.
#          - Drops any missing values.
#          - Filters out points outside specified geographic bounds.
#          - Removes points with large time gaps (taxi stopped too long).
#          - Normalizes the latitude and longitude values.

def preprocess_data(df):
    """Preprocesses the trajectory data."""
    print("Preprocessing data...")
    df = df.sort_values(['taxi_id', 'timestamp'])
    print(f"Missing values before handling: {df.isnull().sum()}")
    df = df.dropna()  # Remove any rows with missing values
    print(f"Missing values after handling: {df.isnull().sum()}")

    # Define geographic bounds (approximate area for Beijing)
    beijing_bounds = {'lat_min': 39.7, 'lat_max': 40.1, 'lon_min': 116.2, 'lon_max': 116.6}
    # Filter rows to keep only points within these bounds
    df = df[(df['latitude'] >= beijing_bounds['lat_min']) & (df['latitude'] <= beijing_bounds['lat_max']) &
            (df['longitude'] >= beijing_bounds['lon_min']) & (df['longitude'] <= beijing_bounds['lon_max'])]

    # Compute time difference between consecutive points for each taxi
    df['time_diff'] = df.groupby('taxi_id')['timestamp'].diff().fillna(0)
    max_stop_time = 300  # Define a threshold of 5 minutes (300 seconds)
    # Filter out points where the taxi stopped for too long
    df = df[df['time_diff'] <= max_stop_time]

    # Normalize the latitude and longitude to a range [0, 1]
    scaler = MinMaxScaler()
    df[['norm_lat', 'norm_lon']] = scaler.fit_transform(df[['latitude', 'longitude']])
    # Save the scaler for later use (e.g., to inverse transform predictions)
    with open("coordinate_scaler.pkl", 'wb') as f:
        pickle.dump(scaler, f)

    # Retain only the necessary columns
    df = df[['taxi_id', 'timestamp', 'latitude', 'longitude', 'norm_lat', 'norm_lon']]
    return df


# Function: create_sequences
# Purpose: Converts the trajectory DataFrame into sequences for RNN input.
#          For each taxi's trajectory, creates input sequences of length seq_length
#          and corresponding output sequences of length pred_horizon.
def create_sequences(df, seq_length=20, pred_horizon=5):
    """Creates input-output sequences for the RNN."""
    print("Creating sequences...")
    # Group data by taxi_id to treat each taxi's trajectory separately
    grouped = df.groupby('taxi_id')
    X, y = [], []  # Lists to hold input and output sequences
    trajectories = 0  # Count of trajectories processed

    # Iterate through each taxi's trajectory
    for _, group in tqdm(grouped):
        # Skip trajectories that are too short
        if len(group) < seq_length + pred_horizon:
            continue
        # Slide over the trajectory to create multiple sequences
        for i in range(len(group) - seq_length - pred_horizon + 1):
            # Input sequence: normalized coordinates for seq_length time steps
            inp_seq = group.iloc[i:i+seq_length][['norm_lat', 'norm_lon']].values
            # Output sequence: next pred_horizon points (normalized coordinates)
            out_seq = group.iloc[i+seq_length:i+seq_length+pred_horizon][['norm_lat', 'norm_lon']].values
            X.append(inp_seq)
            y.append(out_seq)
        trajectories += 1
        if trajectories >= 1100:  # Limit to a certain number of trajectories if needed
            break
    # Convert lists to NumPy arrays for model training
    X = np.array(X)
    y = np.array(y)
    print(f"Created {len(X)} sequences from {trajectories} trajectories")
    print(f"X shape: {X.shape}, y shape: {y.shape}")
    return X, y


# Function: build_lstm_model
# Purpose: Constructs a simple LSTM model with two LSTM layers and dropout.
#          The Dense layer outputs a flat vector which is then reshaped to the target output shape.

def build_lstm_model(input_shape, output_shape, lstm_units=64):
    """Builds a basic LSTM model for trajectory prediction."""
    model = Sequential([
        LSTM(lstm_units, return_sequences=True, input_shape=input_shape),  # First LSTM layer returns sequence output
        Dropout(0.2),  # Dropout for regularization
        LSTM(lstm_units//2),  # Second LSTM layer (smaller size)
        Dropout(0.2),  # Additional dropout
        Dense(output_shape[0] * output_shape[1])  # Fully connected layer to produce the required output size
    ])
    # Reshape the flat output vector into the desired output shape (pred_horizon x 2)
    model.add(tf.keras.layers.Reshape(output_shape))
    # Compile the model with Mean Squared Error loss and Mean Absolute Error metric
    model.compile(optimizer=Adam(learning_rate=0.001),
                  loss=tf.keras.losses.MeanSquaredError(),
                  metrics=[tf.keras.metrics.MeanAbsoluteError()])
    return model


# Function: build_gru_model
# Purpose: Constructs a GRU-based model with similar architecture to the LSTM model.

def build_gru_model(input_shape, output_shape, gru_units=64):
    """Builds a GRU model for trajectory prediction (Variation 1)."""
    model = Sequential([
        GRU(gru_units, return_sequences=True, input_shape=input_shape),  # First GRU layer
        Dropout(0.2),  # Dropout for regularization
        GRU(gru_units//2),  # Second GRU layer
        Dropout(0.2),  # Additional dropout
        Dense(output_shape[0] * output_shape[1])  # Dense layer to produce flat output
    ])
    model.add(tf.keras.layers.Reshape(output_shape))  # Reshape to match output dimensions
    model.compile(optimizer=Adam(learning_rate=0.001),
                  loss=tf.keras.losses.MeanSquaredError(),
                  metrics=[tf.keras.metrics.MeanAbsoluteError()])
    return model


# Function: build_deep_lstm_model
# Purpose: Constructs a deeper LSTM model with three LSTM layers and lower dropout rates.

def build_deep_lstm_model(input_shape, output_shape, lstm_units=32):
    """Builds a deeper LSTM model for trajectory prediction (Variation 2)."""
    model = Sequential([
        LSTM(lstm_units, return_sequences=True, input_shape=input_shape),  # First LSTM layer
        Dropout(0.1),  # Lower dropout
        LSTM(lstm_units*2, return_sequences=True),  # Second LSTM layer with increased units
        Dropout(0.1),  # Lower dropout
        LSTM(lstm_units),  # Third LSTM layer
        Dropout(0.1),  # Lower dropout
        Dense(output_shape[0] * output_shape[1])  # Dense layer to produce flat output vector
    ])
    model.add(tf.keras.layers.Reshape(output_shape))  # Reshape output to target dimensions
    model.compile(optimizer=Adam(learning_rate=0.0005),
                  loss=tf.keras.losses.MeanSquaredError(),
                  metrics=[tf.keras.metrics.MeanAbsoluteError()])
    return model


# Function: train_model
# Purpose: Trains the given model using training and validation data.
#          Uses callbacks for early stopping and to save the best model based on validation loss.

def train_model(model, X_train, y_train, X_val, y_val, model_name="basic_lstm", epochs=30):
    """Trains the model and returns the training history."""
    # Stop training early if the validation loss does not improve for 5 epochs
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    # Save the best model to a file
    model_checkpoint = ModelCheckpoint(f"{model_name}_best.h5", monitor='val_loss', save_best_only=True)
    history = model.fit(X_train, y_train, validation_data=(X_val, y_val),
                        epochs=epochs, batch_size=32, callbacks=[early_stopping, model_checkpoint], verbose=1)
    # Save the final model after training
    model.save(f"{model_name}_final.h5")
    return history

# Function: evaluate_model
# Purpose: Evaluates the trained model on the test set.
#          Computes standard metrics (loss, MAE) and also calculates:
#             - ADE: Average Displacement Error across all predicted points.
#             - FDE: Final Displacement Error at the last predicted point.

def evaluate_model(model, X_test, y_test):
    """Evaluates the model on the test set and returns evaluation metrics."""
    metrics = model.evaluate(X_test, y_test, verbose=0)
    results = {'loss': metrics[0], 'mae': metrics[1]}
    # Predict the outputs for the test set
    y_pred = model.predict(X_test)
    # Calculate the Euclidean distance error for each predicted point
    displacement_error = np.sqrt(np.sum((y_pred - y_test)**2, axis=2))
    results['ade'] = np.mean(displacement_error)  # Average error over all points
    # Calculate error for the final predicted point of each sequence
    final_displacement_error = np.sqrt(np.sum((y_pred[:, -1] - y_test[:, -1])**2, axis=1))
    results['fde'] = np.mean(final_displacement_error)
    return results


# Function: visualize_predictions
# Purpose: Visualizes the predicted trajectory against the ground truth.
#          Denormalizes the coordinates using the saved scaler and plots:
#             - The input trajectory (blue)
#             - The true future trajectory (green)
#             - The predicted future trajectory (red)
#          Also marks the starting and ending points.

def visualize_predictions(model, X_test, y_test, scaler, num_examples=5):
    """Visualizes predictions against ground truth."""
    # Get predictions for a few examples
    y_pred = model.predict(X_test[:num_examples])
    for i in range(num_examples):
        plt.figure(figsize=(10, 6))
        input_seq = X_test[i]
        true_future = y_test[i]
        pred_future = y_pred[i]
        # Denormalize each coordinate from normalized space back to original scale
        input_denorm = np.array([scaler.inverse_transform(input_seq[j].reshape(1, -1))[0]
                                 for j in range(input_seq.shape[0])])
        true_future_denorm = np.array([scaler.inverse_transform(true_future[j].reshape(1, -1))[0]
                                       for j in range(true_future.shape[0])])
        pred_future_denorm = np.array([scaler.inverse_transform(pred_future[j].reshape(1, -1))[0]
                                       for j in range(pred_future.shape[0])])
        # Plot the trajectories: input (blue), true future (green), predicted future (red)
        plt.plot(input_denorm[:, 1], input_denorm[:, 0], 'b-', label='Input Trajectory')
        plt.plot(true_future_denorm[:, 1], true_future_denorm[:, 0], 'g-', label='True Future')
        plt.plot(pred_future_denorm[:, 1], pred_future_denorm[:, 0], 'r-', label='Predicted Future')
        # Mark the starting and ending points
        plt.plot(input_denorm[0, 1], input_denorm[0, 0], 'bo', markersize=10, label='Start')
        plt.plot(true_future_denorm[-1, 1], true_future_denorm[-1, 0], 'go', markersize=10, label='True End')
        plt.plot(pred_future_denorm[-1, 1], pred_future_denorm[-1, 0], 'ro', markersize=10, label='Predicted End')
        plt.title(f'Trajectory Prediction Example {i+1}')
        plt.xlabel('Longitude')
        plt.ylabel('Latitude')
        plt.legend()
        plt.grid(True)
        # Save the plot to a file and also display it
        plt.savefig(f'trajectory_prediction_{i+1}.png')
        plt.show()  # Display the plot inline (e.g., in a Jupyter Notebook)
        plt.close()


# Function: plot_training_history
# Purpose: Plots the model's training and validation loss and MAE over epochs.
#          Displays the plots inline and saves them as an image file.

def plot_training_history(history):
    """Plots training and validation loss and MAE."""
    plt.figure(figsize=(12, 5))
    # Plot loss curves
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    # Plot MAE curves (some models may use key 'mae' instead of 'mean_absolute_error')
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('mean_absolute_error', history.history.get('mae')), label='Training MAE')
    plt.plot(history.history.get('val_mean_absolute_error', history.history.get('val_mae')), label='Validation MAE')
    plt.title('Model MAE')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend()
    plt.tight_layout()
    plt.savefig('training_history.png')
    plt.show()  # Display the plot inline
    plt.close()


# Function: compare_models
# Purpose: Compares different model variations by plotting a bar chart for various metrics.
#          Metrics include loss, MAE, ADE, and FDE.

def compare_models(model_results):
    """Compares different model variations using bar charts for each metric."""
    models = list(model_results.keys())
    metrics_list = ['loss', 'mae', 'ade', 'fde']
    plt.figure(figsize=(15, 10))
    # Create a subplot for each metric
    for i, metric in enumerate(metrics_list):
        plt.subplot(2, 2, i+1)
        values = [model_results[model][metric] for model in models]
        plt.bar(models, values)
        plt.title(f'Model Comparison - {metric.upper()}')
        plt.ylabel(metric.upper())
        plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('model_comparison.png')
    plt.show()  # Display the comparison plot inline
    plt.close()


# Function: create_interactive_map
# Purpose: Creates an interactive map (HTML file) using Folium to visualize a trajectory prediction.
#          It shows the input trajectory, the true future, and the predicted future.

def create_interactive_map(X_test, y_test, y_pred, scaler, example_idx=0):
    """Creates an interactive map visualization of a prediction."""
    # Select a specific example from the test set
    input_seq = X_test[example_idx]
    true_future = y_test[example_idx]
    pred_future = y_pred[example_idx]
    # Denormalize coordinates to original scale
    input_denorm = np.array([scaler.inverse_transform(input_seq[j].reshape(1, -1))[0]
                             for j in range(input_seq.shape[0])])
    true_future_denorm = np.array([scaler.inverse_transform(true_future[j].reshape(1, -1))[0]
                                   for j in range(true_future.shape[0])])
    pred_future_denorm = np.array([scaler.inverse_transform(pred_future[j].reshape(1, -1))[0]
                                   for j in range(pred_future.shape[0])])
    # Initialize the map centered at the starting point
    m = folium.Map(location=[input_denorm[0, 0], input_denorm[0, 1]], zoom_start=15)
    # Add the input trajectory as a blue polyline
    points_input = [[lat, lon] for lat, lon in input_denorm]
    folium.PolyLine(points_input, color='blue', weight=2, opacity=0.8, tooltip='Input Trajectory').add_to(m)
    # Add the true future trajectory as a green polyline
    points_true = [[lat, lon] for lat, lon in true_future_denorm]
    folium.PolyLine(points_true, color='green', weight=2, opacity=0.8, tooltip='True Future').add_to(m)
    # Add the predicted future trajectory as a red polyline
    points_pred = [[lat, lon] for lat, lon in pred_future_denorm]
    folium.PolyLine(points_pred, color='red', weight=2, opacity=0.8, tooltip='Predicted Future').add_to(m)
    # Add markers for start and end points of the trajectories
    folium.Marker([input_denorm[0, 0], input_denorm[0, 1]], popup='Start', icon=folium.Icon(color='blue')).add_to(m)
    folium.Marker([true_future_denorm[-1, 0], true_future_denorm[-1, 1]], popup='True End', icon=folium.Icon(color='green')).add_to(m)
    folium.Marker([pred_future_denorm[-1, 0], pred_future_denorm[-1, 1]], popup='Predicted End', icon=folium.Icon(color='red')).add_to(m)
    # Save the interactive map as an HTML file
    m.save('trajectory_map.html')
    print("Interactive map saved as 'trajectory_map.html'. You can open this file in a web browser.")


# Function: main
# Purpose: Orchestrates the complete trajectory prediction pipeline:
#          - Downloads or creates the dataset.
#          - Loads, preprocesses, and creates sequences.
#          - Splits the data into training, validation, and test sets.
#          - Builds and trains three different models (LSTM, GRU, Deep LSTM).
#          - Evaluates the models and compares their performance.
#          - Visualizes predictions and creates an interactive map.

def main():
    """Main function to execute the trajectory prediction pipeline."""
    # Step 1: Data acquisition (download or create synthetic data)
    data_path = download_dataset()
    # Step 2: Load the dataset into a DataFrame
    df = load_data(data_path)
    # Step 3: Preprocess the data (sorting, cleaning, normalization)
    df = preprocess_data(df)
    # Step 4: Create sequences for model input and output
    seq_length = 20  # Number of time steps for input
    pred_horizon = 5  # Number of time steps to predict
    X, y = create_sequences(df, seq_length=seq_length, pred_horizon=pred_horizon)
    # Step 5: Split data into training, validation, and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)
    print(f"Training set: {X_train.shape}, {y_train.shape}")
    print(f"Validation set: {X_val.shape}, {y_val.shape}")
    print(f"Test set: {X_test.shape}, {y_test.shape}")

    # Define input and output shapes for the models
    input_shape = (seq_length, 2)  # Each time step has 2 features: normalized latitude and longitude
    output_shape = (pred_horizon, 2)  # Predicting 5 future points, each with 2 features


    # Train and evaluate the Basic LSTM Model

    print("\nTraining Basic LSTM Model...")
    lstm_model = build_lstm_model(input_shape, output_shape)
    lstm_model.summary()  # Print model architecture
    lstm_history = train_model(lstm_model, X_train, y_train, X_val, y_val, model_name="basic_lstm")
    plot_training_history(lstm_history)  # Plot training history (loss and MAE)
    print("\nEvaluating Basic LSTM Model...")
    lstm_results = evaluate_model(lstm_model, X_test, y_test)
    print(f"Basic LSTM Results: {lstm_results}")


    # Train and evaluate the GRU Model (Variation 1)

    print("\nTraining GRU Model (Variation 1)...")
    gru_model = build_gru_model(input_shape, output_shape)
    gru_model.summary()  # Print model architecture
    gru_history = train_model(gru_model, X_train, y_train, X_val, y_val, model_name="gru_model")
    plot_training_history(gru_history)  # Plot training history
    print("\nEvaluating GRU Model...")
    gru_results = evaluate_model(gru_model, X_test, y_test)
    print(f"GRU Results: {gru_results}")


    # Train and evaluate the Deep LSTM Model (Variation 2)

    print("\nTraining Deep LSTM Model (Variation 2)...")
    deep_lstm_model = build_deep_lstm_model(input_shape, output_shape)
    deep_lstm_model.summary()  # Print model architecture
    deep_lstm_history = train_model(deep_lstm_model, X_train, y_train, X_val, y_val, model_name="deep_lstm")
    plot_training_history(deep_lstm_history)  # Plot training history
    print("\nEvaluating Deep LSTM Model...")
    deep_lstm_results = evaluate_model(deep_lstm_model, X_test, y_test)
    print(f"Deep LSTM Results: {deep_lstm_results}")


    # Compare all models by plotting key metrics

    model_results = {'Basic LSTM': lstm_results, 'GRU': gru_results, 'Deep LSTM': deep_lstm_results}
    compare_models(model_results)


    # Visualize predictions using the best model (Basic LSTM in this case)
    # Load the saved best model and scaler for denormalization

    best_model = tf.keras.models.load_model("basic_lstm_best.h5")
    with open("coordinate_scaler.pkl", 'rb') as f:
        scaler = pickle.load(f)

    # Predict on the test set and visualize a few predictions
    y_pred = best_model.predict(X_test)
    visualize_predictions(best_model, X_test, y_test, scaler)
    # Create an interactive map for one of the predictions
    create_interactive_map(X_test, y_test, y_pred, scaler)

    print("\nTrajectory prediction analysis complete!")
    return model_results

# Run the pipeline if this script is executed directly
if __name__ == "__main__":
    main()

